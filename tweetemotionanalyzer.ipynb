{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNL7rKfRjrSkMeNx71qqSb+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luckyswaminathan/MusicVibe/blob/main/tweetemotionanalyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "a-zoS4YOORin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b74779f8-8374-4c8d-f474-04b94b652126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Import relevant modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk as nl\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "nl.download('stopwords')\n",
        "nl.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nl.download('wordnet')\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,classification_report\n",
        "import seaborn as sns\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "#from scikitplot.metrics import plot_confusion_matrix\n",
        "\n",
        "\n",
        "# #The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "# The following line improves formatting when ouputting NumPy arrays.\n",
        "np.set_printoptions(linewidth = 200)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#imports csv\n",
        "url = 'https://raw.githubusercontent.com/luckyswaminathan/MusicVibe/main/tweet_emotions.csv'\n",
        "\n",
        "msc_df = pd.read_csv(url)\n",
        "msc_df.drop_duplicates(keep='first')\n",
        "msc_df.drop(columns=\"tweet_id\", inplace = True)\n",
        "msc_df = msc_df.reindex(np.random.permutation(msc_df.index))\n",
        "\n",
        " #defining and removing stop words\n",
        "stopWords = set(stopwords.words('english'))\n",
        "stopWords.add(\".\")\n",
        "stopWords.add(\":\")\n",
        "stopWords.add(\"they're\")\n",
        "\n",
        "\n",
        "\n",
        "## msc_df['content'] = (msc_df['content']).apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stopWords]))\n",
        " #tokenizing each sentence in msc_df \n",
        "\n",
        "tknzr = TweetTokenizer()\n",
        "msc_df['content'] = msc_df['content'].apply(lambda x: re.sub('[^a-zA-Z]',' ', str(x)))\n",
        "msc_df['content'] = msc_df['content'].apply(lambda x: tknzr.tokenize(x.lower()))\n",
        "##msc_df['content'] = (msc_df['content']).apply(word_tokenize)\n",
        "\n",
        "lm = WordNetLemmatizer()\n",
        "msc_df['content'] = msc_df['content'].apply(lambda x: [lm.lemmatize(word) for word in x if word not in (stopWords) and word.find(\"quot\") == -1]) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # splitting the input/output parts\n",
        "\n",
        "\n",
        "\n",
        "def custencoder(df):\n",
        "  df.replace(to_replace=\"anger\", value=-1, inplace=True)\n",
        "  df.replace(to_replace=\"boredom\", value=0, inplace=True)\n",
        "  df.replace(to_replace=\"empty\", value=0, inplace=True)\n",
        "  df.replace(to_replace=\"neutral\", value=0, inplace=True)\n",
        "  df.replace(to_replace=\"enthusiasm\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"fun\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"happiness\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"hate\", value=-1, inplace=True)\n",
        "  df.replace(to_replace=\"love\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"relief\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"sadness\", value=-1, inplace=True)\n",
        "  df.replace(to_replace=\"surprise\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"worry\", value=-1, inplace=True)\n",
        "\n",
        "custencoder(msc_df['sentiment'])\n",
        "\n",
        "\n",
        "rows = msc_df.shape[0]\n",
        "maxTrain =int(rows*0.7)\n",
        "maxVal = int(rows*0.9)\n",
        "\n",
        "\n",
        "words_train = msc_df[0:maxTrain]['content']\n",
        "words_val = msc_df[maxTrain+1:maxVal]['content']\n",
        "words_test = msc_df[maxVal+1:-1]['content']\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "feelings_train = msc_df[0:maxTrain]['sentiment']\n",
        "feelings_val = msc_df[maxTrain+1:maxVal]\n",
        "feelings_test = msc_df[maxVal+1:-1]['sentiment']\n",
        "\n",
        "\n",
        "\n",
        "# word_cloud = \"\"\n",
        "# for row in words_train:\n",
        "#   word_cloud+=\" \".join(row)\n",
        "# wordcloud = WordCloud(width = 1000, height = 500,background_color ='white',min_font_size = 10).generate(word_cloud)\n",
        "# plt.imshow(wordcloud)\n",
        "\n",
        "\n",
        "\n",
        "# sns.countplot(x=msc_df['sentiment'])\n",
        "\n"
      ],
      "metadata": {
        "id": "qWirAIOUPFSd"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using word2vec to vectorize words\n",
        "## using CBOW to model this data set- since the data set is 40000 rows seems to be a better fit \n",
        "## + tweets have a bunch of weird spellings so they are omitted\n",
        "\n",
        "\n",
        "size = 1000\n",
        "window = 5\n",
        "min_count = 10\n",
        "workers = 3\n",
        "sg = 0 \n",
        "\n",
        "\n",
        "stemmed_tokens = pd.Series(msc_df['content']).values\n",
        "# Train the Word2Vec Model\n",
        "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, workers = workers, window = window, sg = sg)\n",
        "w2v_model.save(\"word2vec.model\")\n",
        "\n",
        "\n",
        "print(w2v_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "eBC_jZd_TP6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5327f9a-4528-49ad-caa2-e537303e0838"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=3211, vector_size=100, alpha=0.025>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## working with the word2vec model\n",
        "\n",
        "\n",
        "cb_w2v_model = Word2Vec.load(\"word2vec.model\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdv7_RMQ6t2D",
        "outputId": "349b6f2b-5406-4d41-cc48-3f22c29fe479"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=3211, vector_size=100, alpha=0.025>\n"
          ]
        }
      ]
    }
  ]
}