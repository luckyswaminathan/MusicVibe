{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNvr2wiuVjmhe+TzMH619vT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luckyswaminathan/MusicVibe/blob/main/tweetemotionanalyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "a-zoS4YOORin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8589d1bc-e385-4d33-8b56-e13d49f733df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Import relevant modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import string\n",
        "import os\n",
        "import tqdm\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk as nl\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "nl.download('stopwords')\n",
        "nl.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nl.download('wordnet')\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,classification_report\n",
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "#Import the DecisionTreeeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "#from scikitplot.metrics import plot_confusion_matrix\n",
        "\n",
        "\n",
        "# #The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "# The following line improves formatting when ouputting NumPy arrays.\n",
        "np.set_printoptions(linewidth = 200)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#imports csv\n",
        "url = 'https://raw.githubusercontent.com/luckyswaminathan/MusicVibe/main/tweet_emotions.csv'\n",
        "\n",
        "msc_df = pd.read_csv(url)\n",
        "msc_df.drop_duplicates(keep='first')\n",
        "msc_df.drop(columns=\"tweet_id\", inplace = True)\n",
        "msc_df = msc_df.reindex(np.random.permutation(msc_df.index))\n",
        "\n",
        " #defining and removing stop words\n",
        "stopWords = set(stopwords.words('english'))\n",
        "stopWords.add(\".\")\n",
        "stopWords.add(\":\")\n",
        "stopWords.add(\"they're\")\n",
        "stopWords.add(\"u\")\n",
        "stopWords.add(\"im\")\n",
        "\n",
        "\n",
        "\n",
        "## msc_df['content'] = (msc_df['content']).apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stopWords]))\n",
        " #tokenizing each sentence in msc_df \n",
        "\n",
        "def clean_text(row, options):\n",
        "\n",
        "    if options['lowercase']:\n",
        "        row = row.lower()\n",
        "\n",
        "    if options['decode_utf8']:\n",
        "        txt = BeautifulSoup(row, 'lxml')\n",
        "        row = txt.get_text()\n",
        "\n",
        "    if options['remove_url']:\n",
        "        row = row.replace('http\\S+|www.\\S+', '')\n",
        "\n",
        "    if options['remove_mentions']:\n",
        "        row = row.replace('@[A-Za-z0-9]+', '')\n",
        "\n",
        "    return row\n",
        "\n",
        "clean_config = {\n",
        "    'remove_url': True,\n",
        "    'remove_mentions': True,\n",
        "    'decode_utf8': True,\n",
        "    'lowercase': True\n",
        "    }\n",
        "\n",
        "msc_df['content'] = msc_df['content'].apply(clean_text, args=(clean_config,))\n",
        "\n",
        "\n",
        "tknzr = TweetTokenizer()\n",
        "msc_df['content'] = msc_df['content'].apply(lambda x: re.sub('[^a-zA-Z]',' ', str(x)))\n",
        "msc_df['content'] = msc_df['content'].apply(lambda x: tknzr.tokenize(x.lower()))\n",
        "##msc_df['content'] = (msc_df['content']).apply(word_tokenize)\n",
        "\n",
        "lm = WordNetLemmatizer()\n",
        "msc_df['content'] = msc_df['content'].apply(lambda x: [lm.lemmatize(word) for word in x if word not in (stopWords) and word.find(\"quot\") == -1]) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # splitting the input/output parts\n",
        "\n",
        "\n",
        "\n",
        "def custencoder(df):\n",
        "  df.replace(to_replace=\"anger\", value=-1, inplace=True)\n",
        "  df.replace(to_replace=\"boredom\", value=0, inplace=True)\n",
        "  df.replace(to_replace=\"empty\", value=0, inplace=True)\n",
        "  df.replace(to_replace=\"neutral\", value=0, inplace=True)\n",
        "  df.replace(to_replace=\"enthusiasm\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"fun\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"happiness\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"hate\", value=-1, inplace=True)\n",
        "  df.replace(to_replace=\"love\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"relief\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"sadness\", value=-1, inplace=True)\n",
        "  df.replace(to_replace=\"surprise\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"worry\", value=-1, inplace=True)\n",
        "\n",
        "custencoder(msc_df['sentiment'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rows = msc_df.shape[0]\n",
        "\n",
        "words_train, words_mix = train_test_split(msc_df, test_size=0.2)\n",
        "words_val, words_test = train_test_split(words_mix, test_size=0.33)\n",
        "\n",
        "\n",
        "neg_words = words_train[words_train['sentiment'] == -1]\n",
        "\n",
        "pos_words = words_train[words_train['sentiment'] == 1]\n",
        "neutral_words = words_train[words_train['sentiment'] == 0]\n",
        "# word_cloud = \"\"\n",
        "# for row in words_train[0:-1]['content']:\n",
        "#   word_cloud+=\" \".join(row)\n",
        "# wordcloud = WordCloud(width = 1000, height = 500,background_color ='white',min_font_size = 10).generate(word_cloud)\n",
        "# plt.imshow(wordcloud)\n",
        "\n",
        "\n",
        "# sns.countplot(x=msc_df['sentiment'])\n",
        "\n"
      ],
      "metadata": {
        "id": "qWirAIOUPFSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc07c775-8af4-4e11-cb93-054d801cccea"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-9aca70f25cf1>:28: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  txt = BeautifulSoup(row, 'lxml')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nzko1DL_hDgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using word2vec to vectorize words\n",
        "## using CBOW to model this data set- since the data set is 40000 rows seems to be a better fit \n",
        "## + tweets have a bunch of weird spellings so they are omitted\n",
        "\n",
        "\n",
        "size = 10000\n",
        "window = 5\n",
        "min_count = 10\n",
        "workers = 3\n",
        "sg = 0 \n",
        "output_folder=\"/usr/lakshmanswaminathan/Downloads/musicVibe/\"\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "word2vec_model_file = os.path.join(output_folder, f'word2vec_{size}.model')\n",
        "stemmed_tokens = pd.Series(words_train['content']).values\n",
        "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, workers = workers, window = window, sg = sg)\n",
        "w2v_model.save(word2vec_model_file)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "eBC_jZd_TP6F"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## working with the word2vec model/training the model\n",
        "\n",
        "\n",
        "cb_w2v_model = Word2Vec.load(word2vec_model_file)\n",
        "\n",
        "emb_df = (\n",
        "    pd.DataFrame(\n",
        "        [cb_w2v_model.wv.get_vector(str(n)) for n in cb_w2v_model.wv.key_to_index],\n",
        "        index = cb_w2v_model.wv.key_to_index\n",
        "    )\n",
        ")\n",
        "\n",
        "word2vec_filename = os.path.join(output_folder, 'train_word2vec.csv')\n",
        "\n",
        "with open(word2vec_filename, 'w+') as word2vec_file:\n",
        "    for index, row in words_train.iterrows():\n",
        "        model_vector = (np.mean([cb_w2v_model.wv[token] for token in row['content'] if token in cb_w2v_model.wv], axis=0)).tolist()\n",
        "        if index == 0:\n",
        "            header = \",\".join(str(ele) for ele in range(1000))\n",
        "            word2vec_file.write(header)\n",
        "            word2vec_file.write(\"\\n\")\n",
        "        if type(model_vector) is list:  \n",
        "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
        "        else:\n",
        "            line1 = \",\".join([str(0) for i in range(1000)])\n",
        "        word2vec_file.write(line1)\n",
        "        word2vec_file.write('\\n')\n",
        "\n",
        "## vectors can now be trained\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdv7_RMQ6t2D",
        "outputId": "3fd58020-88ae-4fed-ddf4-120630008202"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## vector training time \n",
        "\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras import layers\n",
        "\n",
        "## building an MLP \n",
        "\n",
        "def create_model(learning_rate):\n",
        "  model = tf.keras.model.Sequential()\n",
        "  model.add(Dense(units=64,input_dim=10000, activation=\"relu\"))\n",
        "\n",
        "  model.add(Dense(units=32,activation='relu'))\n",
        "  model.add(Dense(units=1,activation='sigmoid'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss=)\n",
        "  return model\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RW4V0VOxSfkc"
      },
      "execution_count": 65,
      "outputs": []
    }
  ]
}