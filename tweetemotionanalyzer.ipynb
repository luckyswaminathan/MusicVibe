{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOEZya4lDe0cUHC4W1tWfPO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luckyswaminathan/MusicVibe/blob/main/tweetemotionanalyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "a-zoS4YOORin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e145c940-8e31-4aa3-a148-56b8cd426717"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "#@title Import relevant modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk as nl\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "nl.download('stopwords')\n",
        "nl.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "# The following line improves formatting when ouputting NumPy arrays.\n",
        "np.set_printoptions(linewidth = 200)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#imports csv\n",
        "url = 'https://raw.githubusercontent.com/luckyswaminathan/MusicVibe/main/tweet_emotions.csv'\n",
        "\n",
        "msc_df = pd.read_csv(url)\n",
        "msc_df.drop_duplicates(keep='first')\n",
        "msc_df.drop(columns=\"tweet_id\", inplace = True)\n",
        "msc_df = msc_df.reindex(np.random.permutation(msc_df.index))\n",
        "\n",
        " #defining and removing stop words\n",
        "stopWords = set(stopwords.words('english'))\n",
        "stopWords.add(\".\")\n",
        "stopWords.add(\":\")\n",
        "stopWords.add(\"they're\")\n",
        "\n",
        "\n",
        "\n",
        "## msc_df['content'] = (msc_df['content']).apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stopWords]))\n",
        " #tokenizing each sentence in msc_df \n",
        "\n",
        "tknzr = TweetTokenizer()\n",
        "\n",
        "msc_df['content'] = msc_df['content'].apply(lambda x: tknzr.tokenize(x.lower()))\n",
        "##msc_df['content'] = (msc_df['content']).apply(word_tokenize)\n",
        "\n",
        "\n",
        "msc_df['content'] = msc_df['content'].apply(lambda x: [word for word in x if word not in (stopWords)]) \n",
        "print(msc_df.head())\n",
        "# # splitting the input/output parts\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rows = msc_df.shape[0]\n",
        "maxTrain =int(rows*0.8)\n",
        "\n",
        "words_train = msc_df[0:maxTrain]['content']\n",
        "words_test = msc_df[maxTrain+1:-1]['content']\n",
        "\n",
        "\n",
        "\n",
        "feelings_train = msc_df[0:maxTrain]['sentiment']\n",
        "feelings_test = msc_df[maxTrain+1:-1]['sentiment']\n",
        "\n",
        "\n",
        "# just a print to show the 13 different feelings in this data set\n",
        "print(np.unique(feelings_train))\n"
      ],
      "metadata": {
        "id": "qWirAIOUPFSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1e11b4c-b355-4bbc-912e-3ca4a5c30191"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       sentiment                                            content\n",
            "15094  happiness  [can't, spell, #melo, without, e, ..., <3, fav...\n",
            "35986       love          [salon, mom, happy, mothers, day, mom, !]\n",
            "24664        fun          [@hollaatk, hey, good, weekend, buddy, ?]\n",
            "24792      empty  [@lenasvenson, thought, might, interested, @tw...\n",
            "23637     relief  [@gothtart, congratulations, ,, two, well, sui...\n",
            "['anger' 'boredom' 'empty' 'enthusiasm' 'fun' 'happiness' 'hate' 'love' 'neutral' 'relief' 'sadness' 'surprise' 'worry']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#defining model\n",
        "\n",
        "def createmodel(my_inputs, my_features):\n",
        "  \n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "eBC_jZd_TP6F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133
        },
        "outputId": "60af21a7-83ee-4a6f-d151-c14e22cdf0b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-27-f88373e36ce1>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
          ]
        }
      ]
    }
  ]
}