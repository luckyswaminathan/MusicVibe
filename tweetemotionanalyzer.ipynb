{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO9tsr4VLvKv/J/ZE0GU5Uk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/luckyswaminathan/MusicVibe/blob/main/tweetemotionanalyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-zoS4YOORin",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61f83a80-3ce4-49d7-97a5-ee630e859a07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "#@title Import relevant modules\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import io\n",
        "import string\n",
        "import os\n",
        "import tqdm\n",
        "import time\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "import nltk as nl\n",
        "from nltk import word_tokenize\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "nl.download('stopwords')\n",
        "nl.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "nl.download('wordnet')\n",
        "from wordcloud import WordCloud\n",
        "import re\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,confusion_matrix,roc_curve,classification_report\n",
        "from gensim.models import Word2Vec\n",
        "import time\n",
        "from bs4 import BeautifulSoup\n",
        "#Import the DecisionTreeeClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "#from scikitplot.metrics import plot_confusion_matrix\n",
        "\n",
        "\n",
        "# #The following lines adjust the granularity of reporting. \n",
        "pd.options.display.max_rows = 10\n",
        "pd.options.display.float_format = \"{:.1f}\".format\n",
        "\n",
        "# The following line improves formatting when ouputting NumPy arrays.\n",
        "np.set_printoptions(linewidth = 200)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#imports csv\n",
        "url = 'https://raw.githubusercontent.com/luckyswaminathan/MusicVibe/main/tweet_emotions.csv'\n",
        "\n",
        "msc_df = pd.read_csv(url)\n",
        "msc_df.drop_duplicates(keep='first')\n",
        "msc_df.drop(columns=\"tweet_id\", inplace = True)\n",
        "\n",
        "\n",
        "\n",
        "# # splitting the input/output parts\n",
        "\n",
        "\n",
        "\n",
        "def custencoder(df):\n",
        "  df.replace(to_replace=\"anger\", value=-1, inplace=True)\n",
        "  df.replace(to_replace=\"boredom\", value=0, inplace=True)\n",
        "  df.replace(to_replace=\"empty\", value=0, inplace=True)\n",
        "  df.replace(to_replace=\"neutral\", value=0, inplace=True)\n",
        "  df.replace(to_replace=\"enthusiasm\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"fun\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"happiness\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"hate\", value=-1, inplace=True)\n",
        "  df.replace(to_replace=\"love\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"relief\", value=0, inplace=True)\n",
        "  df.replace(to_replace=\"sadness\", value=-1, inplace=True)\n",
        "  df.replace(to_replace=\"surprise\", value=1, inplace=True)\n",
        "  df.replace(to_replace=\"worry\", value=-1, inplace=True)\n",
        "\n",
        "custencoder(msc_df['sentiment'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rows = msc_df.shape[0]\n",
        "\n",
        "\n",
        "def getTvals(tval=10000):\n",
        "  neg_words = msc_df[msc_df['sentiment'] == -1].head(tval)\n",
        "  pos_words = msc_df[msc_df['sentiment'] == 1].head(tval)\n",
        "  \n",
        "  neutral_words = msc_df[msc_df['sentiment'] == 0].head(tval)\n",
        "  \n",
        "  m = pd.concat([neg_words, pos_words, neutral_words])\n",
        "  return m\n",
        "\n",
        "\n",
        "msc_df = getTvals()\n",
        "\n",
        "msc_df = msc_df.reindex(np.random.permutation(msc_df.index))\n",
        "\n",
        "\n",
        " #defining and removing stop words\n",
        "stopWords = set(stopwords.words('english'))\n",
        "stopWords.add(\".\")\n",
        "stopWords.add(\":\")\n",
        "stopWords.add(\"they're\")\n",
        "stopWords.add(\"u\")\n",
        "stopWords.add(\"im\")\n",
        "\n",
        "\n",
        "\n",
        "## msc_df['content'] = (msc_df['content']).apply(lambda x: ' '.join([word for word in x.split() if word.lower() not in stopWords]))\n",
        " #tokenizing each sentence in msc_df \n",
        "\n",
        "def clean_text(row, options):\n",
        "\n",
        "    if options['lowercase']:\n",
        "        row = row.lower()\n",
        "\n",
        "    if options['decode_utf8']:\n",
        "        txt = BeautifulSoup(row, 'lxml')\n",
        "        row = txt.get_text()\n",
        "\n",
        "    if options['remove_url']:\n",
        "        row = row.replace('http\\S+|www.\\S+', '')\n",
        "\n",
        "    if options['remove_mentions']:\n",
        "        row = row.replace('@[A-Za-z0-9]+', '')\n",
        "\n",
        "    return row\n",
        "\n",
        "clean_config = {\n",
        "    'remove_url': True,\n",
        "    'remove_mentions': True,\n",
        "    'decode_utf8': True,\n",
        "    'lowercase': True\n",
        "    }\n",
        "\n",
        "msc_df['content'] = msc_df['content'].apply(clean_text, args=(clean_config,))\n",
        "\n",
        "\n",
        "tknzr = TweetTokenizer()\n",
        "msc_df['content'] = msc_df['content'].apply(lambda x: re.sub('[^a-zA-Z]',' ', str(x)))\n",
        "msc_df['content'] = msc_df['content'].apply(lambda x: tknzr.tokenize(x.lower()))\n",
        "##msc_df['content'] = (msc_df['content']).apply(word_tokenize)\n",
        "\n",
        "lm = WordNetLemmatizer()\n",
        "msc_df['content'] = msc_df['content'].apply(lambda x: [lm.lemmatize(word) for word in x if word not in (stopWords) and word.find(\"quot\") == -1]) \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "words_train, words_test, feel_train, feel_test = train_test_split(msc_df['content'], msc_df['sentiment'], train_size=24000)\n"
      ],
      "metadata": {
        "id": "qWirAIOUPFSd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc249887-9fd7-491f-ab65-39b4416bb7f8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-69-d51e149f2547>:73: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  txt = BeautifulSoup(row, 'lxml')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nzko1DL_hDgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using word2vec to vectorize words\n",
        "## using CBOW to model this data set- since the data set is 40000 rows seems to be a better fit \n",
        "## + tweets have a bunch of weird spellings so they are omitted\n",
        "\n",
        "\n",
        "size = 1000\n",
        "window = 5\n",
        "min_count = 3\n",
        "workers = 3\n",
        "sg = 0 \n",
        "output_folder=\"/usr/lakshmanswaminathan/Downloads/musicVibe/\"\n",
        "\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "word2vec_model_file = os.path.join(output_folder, f'word2vec_{size}.model')\n",
        "stemmed_tokens = pd.Series(words_train).values\n",
        "\n",
        "\n",
        "w2v_model = Word2Vec(stemmed_tokens, min_count = min_count, vector_size = size , workers = workers, window = window, sg = sg)\n",
        "w2v_model.save(word2vec_model_file)\n",
        "\n",
        "print(w2v_model)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "metadata": {
        "id": "eBC_jZd_TP6F",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b2b888b-e5ad-4cf2-add5-eda3a842c04b"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec<vocab=6136, vector_size=1000, alpha=0.025>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## working with the word2vec model/training the model\n",
        "\n",
        "\n",
        "cb_w2v_model = Word2Vec.load(word2vec_model_file)\n",
        "\n",
        "emb_df = (\n",
        "    pd.DataFrame(\n",
        "        [cb_w2v_model.wv.get_vector(str(n)) for n in cb_w2v_model.wv.key_to_index],\n",
        "        index = cb_w2v_model.wv.key_to_index\n",
        "    )\n",
        ")\n",
        "print(emb_df.info())\n",
        "word2vec_filename = os.path.join(output_folder, 'train_word2vec.csv')\n",
        "\n",
        "with open(word2vec_filename, 'w+') as word2vec_file:\n",
        "    for index, row in msc_df.iterrows():\n",
        "        model_vector = (np.mean([cb_w2v_model.wv[token] for token in row['content'] if token in cb_w2v_model.wv], axis=0)).tolist()\n",
        "        if index == 0:\n",
        "            header = \",\".join(str(ele) for ele in range(size))\n",
        "            word2vec_file.write(header)\n",
        "            word2vec_file.write(\"\\n\")\n",
        "        if type(model_vector) is list:  \n",
        "            line1 = \",\".join( [str(vector_element) for vector_element in model_vector] )\n",
        "        else:\n",
        "            line1 = \",\".join([str(0) for i in range(size)])\n",
        "        word2vec_file.write(line1)\n",
        "        word2vec_file.write('\\n')\n",
        "\n",
        "## vectors can now be trained\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bdv7_RMQ6t2D",
        "outputId": "f7368f85-8d87-4c4e-ca47-ece2db8f7938"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 6136 entries, day to jan\n",
            "Columns: 1000 entries, 0 to 999\n",
            "dtypes: float32(1000)\n",
            "memory usage: 23.5+ MB\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:3474: RuntimeWarning: Mean of empty slice.\n",
            "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(words_train.info())\n",
        "print(words_test.info())\n",
        "print(cb_w2v_model.predict_output_word(\"does this model even work\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7bMjqFLDXUh",
        "outputId": "a704bda2-7cd9-4343-8fb4-182f3cf10911"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.series.Series'>\n",
            "Int64Index: 24000 entries, 31747 to 18099\n",
            "Series name: content\n",
            "Non-Null Count  Dtype \n",
            "--------------  ----- \n",
            "24000 non-null  object\n",
            "dtypes: object(1)\n",
            "memory usage: 375.0+ KB\n",
            "None\n",
            "<class 'pandas.core.series.Series'>\n",
            "Int64Index: 6000 entries, 6667 to 7649\n",
            "Series name: content\n",
            "Non-Null Count  Dtype \n",
            "--------------  ----- \n",
            "6000 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 93.8+ KB\n",
            "None\n",
            "[('please', 0.0003281505), ('say', 0.00032809278), ('look', 0.0003265026), ('x', 0.00032536098), ('bit', 0.00032353494), ('twitter', 0.00032126915), ('guy', 0.00030563993), ('nice', 0.00030466495), ('friend', 0.00029964224), ('school', 0.0002971262)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Convert sentences to word embeddings using Word2Vec model\n",
        "\n",
        "\n",
        "\n",
        "embedding_dim = cb_w2v_model.vector_size\n",
        "\n",
        "# Define the model architecture\n",
        "model = Sequential()\n",
        "model.add(Dense(units=64, input_dim=1000, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(units=32, activation='relu'))\n",
        "model.add(Dense(units=3, activation='softmax'))  # Assuming 3 sentiment classes\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.03),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(emb_df, msc_df['sentiment'], epochs=100, batch_size=5, validation_split=0.2)\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = model.predict(feel_test)\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "accuracy = accuracy_score(feel_test, predicted_labels)\n",
        "print(\"Accuracy: %.2f%%\" % (accuracy * 100))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "kzy3EOk_JoUs",
        "outputId": "123abe1c-bed8-4aef-c766-47af757f154c"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-a54953ce15dc>\u001b[0m in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsc_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m# Evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1051, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\", line 1109, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/engine/compile_utils.py\", line 265, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 142, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 268, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/losses.py\", line 1984, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/backend.py\", line 5559, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n"
          ]
        }
      ]
    }
  ]
}